{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "punL79CN7Ox6"
      },
      "source": [
        "##### Copyright 2020 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_ckMIh7O7s6D"
      },
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph5eir3Pf-3z"
      },
      "source": [
        "# Constructing a Text Generation Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5Uhzt6vVIB2"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l10c03_nlp_constructing_text_generation_model.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7GbGfr_oLCat"
      },
      "source": [
        "Using most of the techniques you've already learned, it's now possible to generate new text by predicting the next word that follows a given seed word. To practice this method, we'll use the [Kaggle Song Lyrics Dataset](https://www.kaggle.com/mousehead/songlyrics)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aHK2CYygXom"
      },
      "source": [
        "## Import TensorFlow and related functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2LmLTREBf5ng"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Other imports for processing data\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmLTO_dpgge9"
      },
      "source": [
        "## Get the Dataset\n",
        "\n",
        "As noted above, we'll utilize the [Song Lyrics dataset](https://www.kaggle.com/mousehead/songlyrics) on Kaggle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "4Bf5FVHfganK",
        "outputId": "cf463e24-0b74-4672-eb7c-d641f2759dbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-31 01:55:32--  https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8\n",
            "Resolving drive.google.com (drive.google.com)... 172.217.194.102, 172.217.194.101, 172.217.194.113, ...\n",
            "Connecting to drive.google.com (drive.google.com)|172.217.194.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 303 See Other\n",
            "Location: https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/v8159lmkv2ounosuvpu3avam43ee6o62/1680227700000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8?uuid=6b69e0ad-1597-489b-887d-eda979760933 [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2023-03-31 01:55:35--  https://doc-04-ak-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/v8159lmkv2ounosuvpu3avam43ee6o62/1680227700000/11118900490791463723/*/1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8?uuid=6b69e0ad-1597-489b-887d-eda979760933\n",
            "Resolving doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)... 74.125.24.132, 2404:6800:4003:c03::84\n",
            "Connecting to doc-04-ak-docs.googleusercontent.com (doc-04-ak-docs.googleusercontent.com)|74.125.24.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 72436445 (69M) [text/csv]\n",
            "Saving to: ‘/tmp/songdata.csv’\n",
            "\n",
            "/tmp/songdata.csv   100%[===================>]  69.08M   227MB/s    in 0.3s    \n",
            "\n",
            "2023-03-31 01:55:35 (227 MB/s) - ‘/tmp/songdata.csv’ saved [72436445/72436445]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget --no-check-certificate \\\n",
        "    https://drive.google.com/uc?id=1LiJFZd41ofrWoBtW-pMYsfz1w8Ny0Bj8 \\\n",
        "    -O /tmp/songdata.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gu1BTzMIS1oy"
      },
      "source": [
        "## **First 10 Songs**\n",
        "\n",
        "Let's first look at just 10 songs from the dataset, and see how things perform."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmb9rGaAUDO-"
      },
      "source": [
        "### Preprocessing\n",
        "\n",
        "Let's perform some basic preprocessing to get rid of punctuation and make everything lowercase. We'll then split the lyrics up by line and tokenize the lyrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2AVAvyF_Vuh5"
      },
      "outputs": [],
      "source": [
        "def tokenize_corpus(corpus, num_words=-1):\n",
        "  # Fit a Tokenizer on the corpus\n",
        "  if num_words > -1:\n",
        "    tokenizer = Tokenizer(num_words=num_words)\n",
        "  else:\n",
        "    tokenizer = Tokenizer()\n",
        "  tokenizer.fit_on_texts(corpus)\n",
        "  return tokenizer\n",
        "\n",
        "def create_lyrics_corpus(dataset, field):\n",
        "  # Remove all other punctuation\n",
        "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n",
        "  # Make it lowercase\n",
        "  dataset[field] = dataset[field].str.lower()\n",
        "  # Make it one long string to split by line\n",
        "  lyrics = dataset[field].str.cat()\n",
        "  corpus = lyrics.split('\\n')\n",
        "  # Remove any trailing whitespace\n",
        "  for l in range(len(corpus)):\n",
        "    corpus[l] = corpus[l].rstrip()\n",
        "  # Remove any empty lines\n",
        "  corpus = [l for l in corpus if l != '']\n",
        "\n",
        "  return corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "apcEXp7WhVBs",
        "outputId": "88c1de20-2ea9-48ff-ed7b-9b8bab0bb4c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'you': 1, 'i': 2, 'and': 3, 'a': 4, 'me': 5, 'the': 6, 'is': 7, 'my': 8, 'to': 9, 'ma': 10, 'it': 11, 'of': 12, 'im': 13, 'your': 14, 'love': 15, 'so': 16, 'as': 17, 'that': 18, 'in': 19, 'andante': 20, 'boomaboomerang': 21, 'make': 22, 'on': 23, 'oh': 24, 'for': 25, 'but': 26, 'new': 27, 'bang': 28, 'its': 29, 'be': 30, 'like': 31, 'know': 32, 'now': 33, 'how': 34, 'could': 35, 'youre': 36, 'sing': 37, 'never': 38, 'no': 39, 'chiquitita': 40, 'can': 41, 'we': 42, 'song': 43, 'had': 44, 'good': 45, 'youll': 46, 'she': 47, 'just': 48, 'girl': 49, 'again': 50, 'will': 51, 'take': 52, 'please': 53, 'let': 54, 'am': 55, 'eyes': 56, 'was': 57, 'always': 58, 'cassandra': 59, 'blue': 60, 'time': 61, 'dont': 62, 'were': 63, 'return': 64, 'once': 65, 'then': 66, 'sorry': 67, 'cryin': 68, 'over': 69, 'feel': 70, 'ever': 71, 'believe': 72, 'what': 73, 'do': 74, 'go': 75, 'all': 76, 'out': 77, 'think': 78, 'every': 79, 'leave': 80, 'look': 81, 'at': 82, 'way': 83, 'one': 84, 'music': 85, 'down': 86, 'our': 87, 'give': 88, 'learn': 89, 'more': 90, 'us': 91, 'would': 92, 'there': 93, 'before': 94, 'when': 95, 'with': 96, 'feeling': 97, 'play': 98, 'cause': 99, 'away': 100, 'here': 101, 'have': 102, 'yes': 103, 'baby': 104, 'get': 105, 'didnt': 106, 'see': 107, 'did': 108, 'closed': 109, 'realized': 110, 'crazy': 111, 'world': 112, 'lord': 113, 'shes': 114, 'kind': 115, 'without': 116, 'if': 117, 'touch': 118, 'strong': 119, 'making': 120, 'such': 121, 'found': 122, 'true': 123, 'stay': 124, 'together': 125, 'thought': 126, 'come': 127, 'they': 128, 'sweet': 129, 'tender': 130, 'sender': 131, 'tune': 132, 'humdehumhum': 133, 'gonna': 134, 'last': 135, 'leaving': 136, 'sleep': 137, 'only': 138, 'saw': 139, 'tell': 140, 'hes': 141, 'her': 142, 'sound': 143, 'tread': 144, 'lightly': 145, 'ground': 146, 'ill': 147, 'show': 148, 'life': 149, 'too': 150, 'used': 151, 'darling': 152, 'meant': 153, 'break': 154, 'end': 155, 'yourself': 156, 'little': 157, 'dumbedumdum': 158, 'bedumbedumdum': 159, 'youve': 160, 'dumbbedumbdumb': 161, 'bedumbbedumbdumb': 162, 'by': 163, 'theyre': 164, 'alone': 165, 'misunderstood': 166, 'day': 167, 'dawning': 168, 'some': 169, 'wanted': 170, 'none': 171, 'listen': 172, 'words': 173, 'warning': 174, 'darkest': 175, 'nights': 176, 'nobody': 177, 'knew': 178, 'fight': 179, 'caught': 180, 'really': 181, 'power': 182, 'dreams': 183, 'weave': 184, 'until': 185, 'final': 186, 'hour': 187, 'morning': 188, 'ship': 189, 'gone': 190, 'grieving': 191, 'still': 192, 'pain': 193, 'cry': 194, 'sun': 195, 'try': 196, 'face': 197, 'something': 198, 'sees': 199, 'makes': 200, 'fine': 201, 'who': 202, 'mine': 203, 'leaves': 204, 'walk': 205, 'hand': 206, 'well': 207, 'about': 208, 'things': 209, 'slow': 210, 'theres': 211, 'talk': 212, 'why': 213, 'up': 214, 'lousy': 215, 'packing': 216, 'ive': 217, 'gotta': 218, 'near': 219, 'keeping': 220, 'intention': 221, 'growing': 222, 'taking': 223, 'dimension': 224, 'even': 225, 'better': 226, 'thank': 227, 'god': 228, 'not': 229, 'somebody': 230, 'happy': 231, 'question': 232, 'smile': 233, 'mean': 234, 'much': 235, 'kisses': 236, 'around': 237, 'anywhere': 238, 'advice': 239, 'care': 240, 'use': 241, 'selfish': 242, 'tool': 243, 'fool': 244, 'showing': 245, 'boomerang': 246, 'throwing': 247, 'warm': 248, 'kiss': 249, 'surrender': 250, 'giving': 251, 'been': 252, 'door': 253, 'burning': 254, 'bridges': 255, 'being': 256, 'moving': 257, 'though': 258, 'behind': 259, 'are': 260, 'must': 261, 'sure': 262, 'stood': 263, 'hope': 264, 'this': 265, 'deny': 266, 'sad': 267, 'quiet': 268, 'truth': 269, 'heartaches': 270, 'scars': 271, 'dancing': 272, 'sky': 273, 'shining': 274, 'above': 275, 'hear': 276, 'came': 277, 'couldnt': 278, 'everything': 279, 'back': 280, 'long': 281, 'waitin': 282, 'cold': 283, 'chills': 284, 'bone': 285, 'youd': 286, 'wonderful': 287, 'means': 288, 'special': 289, 'smiles': 290, 'lucky': 291, 'fellow': 292, 'park': 293, 'holds': 294, 'squeezes': 295, 'walking': 296, 'hours': 297, 'talking': 298, 'plan': 299, 'easy': 300, 'gently': 301, 'summer': 302, 'evening': 303, 'breeze': 304, 'grow': 305, 'fingers': 306, 'soft': 307, 'light': 308, 'body': 309, 'velvet': 310, 'night': 311, 'soul': 312, 'slowly': 313, 'shimmer': 314, 'thousand': 315, 'butterflies': 316, 'float': 317, 'put': 318, 'rotten': 319, 'boy': 320, 'tough': 321, 'stuff': 322, 'saying': 323, 'need': 324, 'anymore': 325, 'enough': 326, 'standing': 327, 'creep': 328, 'felt': 329, 'cheap': 330, 'notion': 331, 'deep': 332, 'dumb': 333, 'mistake': 334, 'entitled': 335, 'another': 336, 'beg': 337, 'forgive': 338, 'an': 339, 'feels': 340, 'hoot': 341, 'holler': 342, 'mad': 343, 'under': 344, 'heel': 345, 'holy': 346, 'christ': 347, 'deal': 348, 'sick': 349, 'tired': 350, 'tedious': 351, 'ways': 352, 'aint': 353, 'walkin': 354, 'cutting': 355, 'tie': 356, 'wanna': 357, 'into': 358, 'eye': 359, 'myself': 360, 'counting': 361, 'pride': 362, 'unright': 363, 'neighbours': 364, 'ride': 365, 'burying': 366, 'past': 367, 'peace': 368, 'free': 369, 'sucker': 370, 'street': 371, 'singing': 372, 'shouting': 373, 'staying': 374, 'alive': 375, 'city': 376, 'dead': 377, 'hiding': 378, 'their': 379, 'shame': 380, 'hollow': 381, 'laughter': 382, 'while': 383, 'crying': 384, 'bed': 385, 'pity': 386, 'believed': 387, 'lost': 388, 'from': 389, 'start': 390, 'suffer': 391, 'sell': 392, 'secrets': 393, 'bargain': 394, 'playing': 395, 'smart': 396, 'aching': 397, 'hearts': 398, 'sailing': 399, 'father': 400, 'sister': 401, 'reason': 402, 'linger': 403, 'deeply': 404, 'future': 405, 'casting': 406, 'shadow': 407, 'else': 408, 'fate': 409, 'bags': 410, 'thorough': 411, 'knowing': 412, 'late': 413, 'wait': 414, 'watched': 415, 'harbor': 416, 'sunrise': 417, 'sails': 418, 'almost': 419, 'slack': 420, 'cool': 421, 'rain': 422, 'deck': 423, 'tiny': 424, 'figure': 425, 'rigid': 426, 'restrained': 427, 'filled': 428, 'whats': 429, 'wrong': 430, 'enchained': 431, 'own': 432, 'sorrow': 433, 'tomorrow': 434, 'hate': 435, 'shoulder': 436, 'best': 437, 'friend': 438, 'rely': 439, 'broken': 440, 'feather': 441, 'patch': 442, 'walls': 443, 'tumbling': 444, 'loves': 445, 'blown': 446, 'candle': 447, 'seems': 448, 'hard': 449, 'handle': 450, 'id': 451, 'thinking': 452, 'went': 453, 'house': 454, 'hardly': 455, 'guy': 456, 'closing': 457, 'front': 458, 'emptiness': 459, 'he': 460, 'disapeared': 461, 'his': 462, 'car': 463, 'stunned': 464, 'dreamed': 465, 'lifes': 466, 'part': 467, 'move': 468, 'feet': 469, 'pavement': 470, 'acted': 471, 'told': 472, 'lies': 473, 'meet': 474, 'other': 475, 'guys': 476, 'stupid': 477, 'blind': 478, 'smiled': 479, 'took': 480, 'said': 481, 'may': 482, 'couple': 483, 'men': 484, 'them': 485, 'brother': 486, 'joe': 487, 'seeing': 488, 'lot': 489, 'him': 490, 'nice': 491, 'sitting': 492, 'sittin': 493, 'memories': 494}\n",
            "495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-fbdddccf8583>:12: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  dataset[field] = dataset[field].str.replace('[{}]'.format(string.punctuation), '')\n"
          ]
        }
      ],
      "source": [
        "# Read the dataset from csv - just first 10 songs for now\n",
        "dataset = pd.read_csv('/tmp/songdata.csv', dtype=str)[:10]\n",
        "# Create the corpus using the 'text' column containing lyrics\n",
        "corpus = create_lyrics_corpus(dataset, 'text')\n",
        "# Tokenize the corpus\n",
        "tokenizer = tokenize_corpus(corpus)\n",
        "\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "\n",
        "print(tokenizer.word_index)\n",
        "print(total_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9x68iN_X6FK"
      },
      "source": [
        "### Create Sequences and Labels\n",
        "\n",
        "After preprocessing, we next need to create sequences and labels. Creating the sequences themselves is similar to before with `texts_to_sequences`, but also including the use of [N-Grams](https://towardsdatascience.com/introduction-to-language-models-n-gram-e323081503d9); creating the labels will now utilize those sequences as well as utilize one-hot encoding over all potential output words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "QmlTsUqfikVO"
      },
      "outputs": [],
      "source": [
        "sequences = []\n",
        "for line in corpus:\n",
        "\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n",
        "\tfor i in range(1, len(token_list)):\n",
        "\t\tn_gram_sequence = token_list[:i+1]\n",
        "\t\tsequences.append(n_gram_sequence)\n",
        "\n",
        "# Pad sequences for equal input length \n",
        "max_sequence_len = max([len(seq) for seq in sequences])\n",
        "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "\n",
        "# Split sequences between the \"input\" sequence and \"output\" predicted word\n",
        "input_sequences, labels = sequences[:,:-1], sequences[:,-1]\n",
        "# One-hot encode the labels\n",
        "one_hot_labels = tf.keras.utils.to_categorical(labels, num_classes=total_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Zsmu3aEId49i",
        "outputId": "2a38e480-53aa-4b50-ee9d-b88292538c3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32\n",
            "97\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29\n",
            "   4]\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0  81  82 142 197  29   4\n",
            " 287]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
          ]
        }
      ],
      "source": [
        "# Check out how some of our data is being stored\n",
        "# The Tokenizer has just a single index per word\n",
        "print(tokenizer.word_index['know'])\n",
        "print(tokenizer.word_index['feeling'])\n",
        "# Input sequences will have multiple indexes\n",
        "print(input_sequences[5])\n",
        "print(input_sequences[6])\n",
        "# And the one hot labels will be as long as the full spread of tokenized words\n",
        "print(one_hot_labels[5])\n",
        "print(one_hot_labels[6])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1TAJMlmfO8r"
      },
      "source": [
        "### Train a Text Generation Model\n",
        "\n",
        "Building an RNN to train our text generation model will be very similar to the sentiment models you've built previously. The only real change necessary is to make sure to use Categorical instead of Binary Cross Entropy as the loss function - we could use Binary before since the sentiment was only 0 or 1, but now there are hundreds of categories.\n",
        "\n",
        "From there, we should also consider using *more* epochs than before, as text generation can take a little longer to converge than sentiment analysis, *and* we aren't working with all that much data yet. I'll set it at 200 epochs here since we're only use part of the dataset, and training will tail off quite a bit over that many epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "G1YXuxIqfygN",
        "outputId": "746ff025-3e1e-4ef7-d5c9-8a1c94e9aa3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "62/62 [==============================] - 18s 81ms/step - loss: 5.7959 - accuracy: 0.0343\n",
            "Epoch 2/300\n",
            "62/62 [==============================] - 2s 36ms/step - loss: 5.4067 - accuracy: 0.0419\n",
            "Epoch 3/300\n",
            "62/62 [==============================] - 1s 19ms/step - loss: 5.2798 - accuracy: 0.0383\n",
            "Epoch 4/300\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 5.1249 - accuracy: 0.0479\n",
            "Epoch 5/300\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 4.9680 - accuracy: 0.0570\n",
            "Epoch 6/300\n",
            "62/62 [==============================] - 1s 12ms/step - loss: 4.8067 - accuracy: 0.0807\n",
            "Epoch 7/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 4.6253 - accuracy: 0.0974\n",
            "Epoch 8/300\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 4.3885 - accuracy: 0.1231\n",
            "Epoch 9/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 4.1561 - accuracy: 0.1685\n",
            "Epoch 10/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 3.8870 - accuracy: 0.2064\n",
            "Epoch 11/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 3.6407 - accuracy: 0.2361\n",
            "Epoch 12/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 3.3838 - accuracy: 0.2876\n",
            "Epoch 13/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 3.1802 - accuracy: 0.3239\n",
            "Epoch 14/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 2.9556 - accuracy: 0.3718\n",
            "Epoch 15/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.7706 - accuracy: 0.4137\n",
            "Epoch 16/300\n",
            "62/62 [==============================] - 1s 15ms/step - loss: 2.5719 - accuracy: 0.4561\n",
            "Epoch 17/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 2.3921 - accuracy: 0.4864\n",
            "Epoch 18/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 2.2326 - accuracy: 0.5242\n",
            "Epoch 19/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 2.0711 - accuracy: 0.5515\n",
            "Epoch 20/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 1.9496 - accuracy: 0.5782\n",
            "Epoch 21/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 1.8140 - accuracy: 0.6070\n",
            "Epoch 22/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 1.7185 - accuracy: 0.6307\n",
            "Epoch 23/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.6335 - accuracy: 0.6418\n",
            "Epoch 24/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.4858 - accuracy: 0.6801\n",
            "Epoch 25/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.3973 - accuracy: 0.6988\n",
            "Epoch 26/300\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 1.3089 - accuracy: 0.7210\n",
            "Epoch 27/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.2385 - accuracy: 0.7402\n",
            "Epoch 28/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 1.1600 - accuracy: 0.7634\n",
            "Epoch 29/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 1.0877 - accuracy: 0.7790\n",
            "Epoch 30/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 1.0197 - accuracy: 0.7931\n",
            "Epoch 31/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.9594 - accuracy: 0.8007\n",
            "Epoch 32/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.9048 - accuracy: 0.8073\n",
            "Epoch 33/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.8562 - accuracy: 0.8254\n",
            "Epoch 34/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.8104 - accuracy: 0.8401\n",
            "Epoch 35/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.7640 - accuracy: 0.8431\n",
            "Epoch 36/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.7859 - accuracy: 0.8340\n",
            "Epoch 37/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.7697 - accuracy: 0.8335\n",
            "Epoch 38/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.7068 - accuracy: 0.8481\n",
            "Epoch 39/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.6297 - accuracy: 0.8673\n",
            "Epoch 40/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.5922 - accuracy: 0.8759\n",
            "Epoch 41/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.5643 - accuracy: 0.8789\n",
            "Epoch 42/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.5517 - accuracy: 0.8819\n",
            "Epoch 43/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.5278 - accuracy: 0.8855\n",
            "Epoch 44/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.5103 - accuracy: 0.8860\n",
            "Epoch 45/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4935 - accuracy: 0.8865\n",
            "Epoch 46/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4798 - accuracy: 0.8870\n",
            "Epoch 47/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.4688 - accuracy: 0.8865\n",
            "Epoch 48/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.4448 - accuracy: 0.8905\n",
            "Epoch 49/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4171 - accuracy: 0.8966\n",
            "Epoch 50/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.4064 - accuracy: 0.8981\n",
            "Epoch 51/300\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.3918 - accuracy: 0.9001\n",
            "Epoch 52/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.3868 - accuracy: 0.8976\n",
            "Epoch 53/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.3680 - accuracy: 0.8971\n",
            "Epoch 54/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.3632 - accuracy: 0.9001\n",
            "Epoch 55/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3479 - accuracy: 0.9016\n",
            "Epoch 56/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3429 - accuracy: 0.8996\n",
            "Epoch 57/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.3385 - accuracy: 0.9026\n",
            "Epoch 58/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3323 - accuracy: 0.8991\n",
            "Epoch 59/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.3254 - accuracy: 0.9067\n",
            "Epoch 60/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.3141 - accuracy: 0.9026\n",
            "Epoch 61/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3131 - accuracy: 0.9041\n",
            "Epoch 62/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.3169 - accuracy: 0.9016\n",
            "Epoch 63/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3130 - accuracy: 0.9001\n",
            "Epoch 64/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.3182 - accuracy: 0.9041\n",
            "Epoch 65/300\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.3066 - accuracy: 0.9041\n",
            "Epoch 66/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2967 - accuracy: 0.9031\n",
            "Epoch 67/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.2951 - accuracy: 0.8986\n",
            "Epoch 68/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.2796 - accuracy: 0.9006\n",
            "Epoch 69/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2761 - accuracy: 0.9082\n",
            "Epoch 70/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2730 - accuracy: 0.9011\n",
            "Epoch 71/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2698 - accuracy: 0.9036\n",
            "Epoch 72/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2693 - accuracy: 0.9026\n",
            "Epoch 73/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2694 - accuracy: 0.9046\n",
            "Epoch 74/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.2697 - accuracy: 0.9051\n",
            "Epoch 75/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.2677 - accuracy: 0.9051\n",
            "Epoch 76/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.2591 - accuracy: 0.9031\n",
            "Epoch 77/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2544 - accuracy: 0.9057\n",
            "Epoch 78/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2522 - accuracy: 0.9072\n",
            "Epoch 79/300\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.2487 - accuracy: 0.9041\n",
            "Epoch 80/300\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.2560 - accuracy: 0.9057\n",
            "Epoch 81/300\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.2765 - accuracy: 0.8940\n",
            "Epoch 82/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2706 - accuracy: 0.9046\n",
            "Epoch 83/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2588 - accuracy: 0.9067\n",
            "Epoch 84/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.2561 - accuracy: 0.9072\n",
            "Epoch 85/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2542 - accuracy: 0.9021\n",
            "Epoch 86/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.2485 - accuracy: 0.9026\n",
            "Epoch 87/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.2437 - accuracy: 0.9057\n",
            "Epoch 88/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2466 - accuracy: 0.9041\n",
            "Epoch 89/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2415 - accuracy: 0.9021\n",
            "Epoch 90/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2376 - accuracy: 0.9062\n",
            "Epoch 91/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2364 - accuracy: 0.9082\n",
            "Epoch 92/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.2324 - accuracy: 0.9021\n",
            "Epoch 93/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2465 - accuracy: 0.9016\n",
            "Epoch 94/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.2456 - accuracy: 0.9036\n",
            "Epoch 95/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.2557 - accuracy: 0.8996\n",
            "Epoch 96/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2446 - accuracy: 0.8991\n",
            "Epoch 97/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.2427 - accuracy: 0.9031\n",
            "Epoch 98/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2360 - accuracy: 0.9011\n",
            "Epoch 99/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.2460 - accuracy: 0.8986\n",
            "Epoch 100/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2604 - accuracy: 0.8976\n",
            "Epoch 101/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3507 - accuracy: 0.8784\n",
            "Epoch 102/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.3090 - accuracy: 0.8845\n",
            "Epoch 103/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2720 - accuracy: 0.8966\n",
            "Epoch 104/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.2448 - accuracy: 0.9036\n",
            "Epoch 105/300\n",
            "62/62 [==============================] - 1s 17ms/step - loss: 0.2349 - accuracy: 0.9072\n",
            "Epoch 106/300\n",
            "62/62 [==============================] - 1s 14ms/step - loss: 0.2369 - accuracy: 0.9051\n",
            "Epoch 107/300\n",
            "62/62 [==============================] - 1s 16ms/step - loss: 0.2348 - accuracy: 0.9026\n",
            "Epoch 108/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.2293 - accuracy: 0.9051\n",
            "Epoch 109/300\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.2273 - accuracy: 0.9057\n",
            "Epoch 110/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2245 - accuracy: 0.9051\n",
            "Epoch 111/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.2272 - accuracy: 0.9062\n",
            "Epoch 112/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.2228 - accuracy: 0.9072\n",
            "Epoch 113/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2243 - accuracy: 0.9016\n",
            "Epoch 114/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2232 - accuracy: 0.9057\n",
            "Epoch 115/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2210 - accuracy: 0.9031\n",
            "Epoch 116/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2221 - accuracy: 0.9087\n",
            "Epoch 117/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2281 - accuracy: 0.9036\n",
            "Epoch 118/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2640 - accuracy: 0.8986\n",
            "Epoch 119/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2367 - accuracy: 0.9006\n",
            "Epoch 120/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2270 - accuracy: 0.9057\n",
            "Epoch 121/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2223 - accuracy: 0.9016\n",
            "Epoch 122/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2199 - accuracy: 0.9031\n",
            "Epoch 123/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.2198 - accuracy: 0.9021\n",
            "Epoch 124/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2191 - accuracy: 0.9082\n",
            "Epoch 125/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2199 - accuracy: 0.9067\n",
            "Epoch 126/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.2222 - accuracy: 0.9006\n",
            "Epoch 127/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2184 - accuracy: 0.9067\n",
            "Epoch 128/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.2198 - accuracy: 0.9026\n",
            "Epoch 129/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2198 - accuracy: 0.9016\n",
            "Epoch 130/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2226 - accuracy: 0.9026\n",
            "Epoch 131/300\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.2224 - accuracy: 0.9062\n",
            "Epoch 132/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.2217 - accuracy: 0.9001\n",
            "Epoch 133/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.2208 - accuracy: 0.9006\n",
            "Epoch 134/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.2195 - accuracy: 0.9041\n",
            "Epoch 135/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2195 - accuracy: 0.9057\n",
            "Epoch 136/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2174 - accuracy: 0.9051\n",
            "Epoch 137/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2128 - accuracy: 0.9092\n",
            "Epoch 138/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2445 - accuracy: 0.8961\n",
            "Epoch 139/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2634 - accuracy: 0.8986\n",
            "Epoch 140/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2536 - accuracy: 0.8996\n",
            "Epoch 141/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2397 - accuracy: 0.8961\n",
            "Epoch 142/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2261 - accuracy: 0.9031\n",
            "Epoch 143/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2175 - accuracy: 0.9051\n",
            "Epoch 144/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2160 - accuracy: 0.9036\n",
            "Epoch 145/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2165 - accuracy: 0.9041\n",
            "Epoch 146/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2163 - accuracy: 0.9051\n",
            "Epoch 147/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.2142 - accuracy: 0.9011\n",
            "Epoch 148/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.2132 - accuracy: 0.9041\n",
            "Epoch 149/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2155 - accuracy: 0.9062\n",
            "Epoch 150/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2127 - accuracy: 0.9041\n",
            "Epoch 151/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2134 - accuracy: 0.9026\n",
            "Epoch 152/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2129 - accuracy: 0.9062\n",
            "Epoch 153/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.2123 - accuracy: 0.9077\n",
            "Epoch 154/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.2110 - accuracy: 0.9072\n",
            "Epoch 155/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2099 - accuracy: 0.9062\n",
            "Epoch 156/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2114 - accuracy: 0.9006\n",
            "Epoch 157/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2107 - accuracy: 0.9057\n",
            "Epoch 158/300\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.2107 - accuracy: 0.9087\n",
            "Epoch 159/300\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.2102 - accuracy: 0.9026\n",
            "Epoch 160/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.2080 - accuracy: 0.9041\n",
            "Epoch 161/300\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.2084 - accuracy: 0.9021\n",
            "Epoch 162/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2084 - accuracy: 0.9036\n",
            "Epoch 163/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.2083 - accuracy: 0.9051\n",
            "Epoch 164/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2068 - accuracy: 0.9072\n",
            "Epoch 165/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2084 - accuracy: 0.9072\n",
            "Epoch 166/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2085 - accuracy: 0.9051\n",
            "Epoch 167/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.2091 - accuracy: 0.9072\n",
            "Epoch 168/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2094 - accuracy: 0.9026\n",
            "Epoch 169/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.2128 - accuracy: 0.9041\n",
            "Epoch 170/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2130 - accuracy: 0.9062\n",
            "Epoch 171/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2098 - accuracy: 0.9057\n",
            "Epoch 172/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2168 - accuracy: 0.9041\n",
            "Epoch 173/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.2099 - accuracy: 0.9077\n",
            "Epoch 174/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2156 - accuracy: 0.9057\n",
            "Epoch 175/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2142 - accuracy: 0.9041\n",
            "Epoch 176/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.2213 - accuracy: 0.9036\n",
            "Epoch 177/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2132 - accuracy: 0.9036\n",
            "Epoch 178/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2137 - accuracy: 0.9031\n",
            "Epoch 179/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2106 - accuracy: 0.9046\n",
            "Epoch 180/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2105 - accuracy: 0.9021\n",
            "Epoch 181/300\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.2109 - accuracy: 0.9082\n",
            "Epoch 182/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2128 - accuracy: 0.9057\n",
            "Epoch 183/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2155 - accuracy: 0.9011\n",
            "Epoch 184/300\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2106 - accuracy: 0.9031\n",
            "Epoch 185/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.2151 - accuracy: 0.9062\n",
            "Epoch 186/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.2147 - accuracy: 0.9057\n",
            "Epoch 187/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.2143 - accuracy: 0.9062\n",
            "Epoch 188/300\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.2178 - accuracy: 0.9041\n",
            "Epoch 189/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2128 - accuracy: 0.9031\n",
            "Epoch 190/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2081 - accuracy: 0.8991\n",
            "Epoch 191/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2094 - accuracy: 0.9102\n",
            "Epoch 192/300\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.2081 - accuracy: 0.9031\n",
            "Epoch 193/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.2078 - accuracy: 0.9102\n",
            "Epoch 194/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.2075 - accuracy: 0.9016\n",
            "Epoch 195/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.2085 - accuracy: 0.9041\n",
            "Epoch 196/300\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2055 - accuracy: 0.9036\n",
            "Epoch 197/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2067 - accuracy: 0.9092\n",
            "Epoch 198/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2060 - accuracy: 0.9021\n",
            "Epoch 199/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.2054 - accuracy: 0.8996\n",
            "Epoch 200/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2065 - accuracy: 0.9067\n",
            "Epoch 201/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.2046 - accuracy: 0.9062\n",
            "Epoch 202/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2042 - accuracy: 0.9062\n",
            "Epoch 203/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2043 - accuracy: 0.9087\n",
            "Epoch 204/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.2026 - accuracy: 0.9051\n",
            "Epoch 205/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2058 - accuracy: 0.9092\n",
            "Epoch 206/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2028 - accuracy: 0.9062\n",
            "Epoch 207/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.2045 - accuracy: 0.9072\n",
            "Epoch 208/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2046 - accuracy: 0.9016\n",
            "Epoch 209/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2038 - accuracy: 0.9077\n",
            "Epoch 210/300\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.2039 - accuracy: 0.9031\n",
            "Epoch 211/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.2035 - accuracy: 0.9077\n",
            "Epoch 212/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.2052 - accuracy: 0.9107\n",
            "Epoch 213/300\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.2050 - accuracy: 0.9051\n",
            "Epoch 214/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2027 - accuracy: 0.9072\n",
            "Epoch 215/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2025 - accuracy: 0.9062\n",
            "Epoch 216/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.2038 - accuracy: 0.9046\n",
            "Epoch 217/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2038 - accuracy: 0.9046\n",
            "Epoch 218/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2038 - accuracy: 0.9082\n",
            "Epoch 219/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2030 - accuracy: 0.9092\n",
            "Epoch 220/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2092 - accuracy: 0.8996\n",
            "Epoch 221/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2089 - accuracy: 0.9051\n",
            "Epoch 222/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.2096 - accuracy: 0.9041\n",
            "Epoch 223/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2110 - accuracy: 0.9041\n",
            "Epoch 224/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2114 - accuracy: 0.9041\n",
            "Epoch 225/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2064 - accuracy: 0.9051\n",
            "Epoch 226/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.2050 - accuracy: 0.9036\n",
            "Epoch 227/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.2008 - accuracy: 0.9077\n",
            "Epoch 228/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2049 - accuracy: 0.9051\n",
            "Epoch 229/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2017 - accuracy: 0.9006\n",
            "Epoch 230/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2018 - accuracy: 0.9062\n",
            "Epoch 231/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.2059 - accuracy: 0.9036\n",
            "Epoch 232/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2097 - accuracy: 0.9072\n",
            "Epoch 233/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2036 - accuracy: 0.9041\n",
            "Epoch 234/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2016 - accuracy: 0.9072\n",
            "Epoch 235/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2015 - accuracy: 0.9046\n",
            "Epoch 236/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.2005 - accuracy: 0.9082\n",
            "Epoch 237/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2067 - accuracy: 0.8996\n",
            "Epoch 238/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.2011 - accuracy: 0.9087\n",
            "Epoch 239/300\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.2016 - accuracy: 0.9062\n",
            "Epoch 240/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.2015 - accuracy: 0.9036\n",
            "Epoch 241/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.2017 - accuracy: 0.9077\n",
            "Epoch 242/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2000 - accuracy: 0.9072\n",
            "Epoch 243/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2022 - accuracy: 0.9026\n",
            "Epoch 244/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.1996 - accuracy: 0.9062\n",
            "Epoch 245/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2021 - accuracy: 0.9041\n",
            "Epoch 246/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2019 - accuracy: 0.9092\n",
            "Epoch 247/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2002 - accuracy: 0.9062\n",
            "Epoch 248/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2031 - accuracy: 0.9062\n",
            "Epoch 249/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2064 - accuracy: 0.9006\n",
            "Epoch 250/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2054 - accuracy: 0.9031\n",
            "Epoch 251/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2023 - accuracy: 0.9062\n",
            "Epoch 252/300\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.2021 - accuracy: 0.9057\n",
            "Epoch 253/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2009 - accuracy: 0.9067\n",
            "Epoch 254/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2007 - accuracy: 0.9117\n",
            "Epoch 255/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2005 - accuracy: 0.9102\n",
            "Epoch 256/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2020 - accuracy: 0.9036\n",
            "Epoch 257/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2035 - accuracy: 0.9046\n",
            "Epoch 258/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2119 - accuracy: 0.9051\n",
            "Epoch 259/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2125 - accuracy: 0.9077\n",
            "Epoch 260/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2047 - accuracy: 0.9046\n",
            "Epoch 261/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2018 - accuracy: 0.9077\n",
            "Epoch 262/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.2021 - accuracy: 0.9067\n",
            "Epoch 263/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2000 - accuracy: 0.9092\n",
            "Epoch 264/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.1994 - accuracy: 0.9092\n",
            "Epoch 265/300\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.1980 - accuracy: 0.9072\n",
            "Epoch 266/300\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.1988 - accuracy: 0.9062\n",
            "Epoch 267/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.1993 - accuracy: 0.9077\n",
            "Epoch 268/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.1987 - accuracy: 0.9077\n",
            "Epoch 269/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.1983 - accuracy: 0.9082\n",
            "Epoch 270/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2003 - accuracy: 0.9077\n",
            "Epoch 271/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.1992 - accuracy: 0.9102\n",
            "Epoch 272/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.1996 - accuracy: 0.9062\n",
            "Epoch 273/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.1996 - accuracy: 0.9107\n",
            "Epoch 274/300\n",
            "62/62 [==============================] - 1s 10ms/step - loss: 0.1983 - accuracy: 0.9082\n",
            "Epoch 275/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.1985 - accuracy: 0.9092\n",
            "Epoch 276/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.1990 - accuracy: 0.9031\n",
            "Epoch 277/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.1987 - accuracy: 0.9067\n",
            "Epoch 278/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.1970 - accuracy: 0.9072\n",
            "Epoch 279/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.1996 - accuracy: 0.9087\n",
            "Epoch 280/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.1999 - accuracy: 0.9087\n",
            "Epoch 281/300\n",
            "62/62 [==============================] - 0s 6ms/step - loss: 0.2010 - accuracy: 0.9107\n",
            "Epoch 282/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.1987 - accuracy: 0.9036\n",
            "Epoch 283/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2060 - accuracy: 0.9021\n",
            "Epoch 284/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2026 - accuracy: 0.9041\n",
            "Epoch 285/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2293 - accuracy: 0.9021\n",
            "Epoch 286/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2223 - accuracy: 0.9016\n",
            "Epoch 287/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2080 - accuracy: 0.9067\n",
            "Epoch 288/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2015 - accuracy: 0.9097\n",
            "Epoch 289/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2017 - accuracy: 0.9112\n",
            "Epoch 290/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.1984 - accuracy: 0.9087\n",
            "Epoch 291/300\n",
            "62/62 [==============================] - 0s 8ms/step - loss: 0.1995 - accuracy: 0.9092\n",
            "Epoch 292/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.1995 - accuracy: 0.9097\n",
            "Epoch 293/300\n",
            "62/62 [==============================] - 1s 9ms/step - loss: 0.1988 - accuracy: 0.9107\n",
            "Epoch 294/300\n",
            "62/62 [==============================] - 1s 11ms/step - loss: 0.1980 - accuracy: 0.9107\n",
            "Epoch 295/300\n",
            "62/62 [==============================] - 1s 8ms/step - loss: 0.1990 - accuracy: 0.9077\n",
            "Epoch 296/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.2000 - accuracy: 0.9087\n",
            "Epoch 297/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.1989 - accuracy: 0.9072\n",
            "Epoch 298/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.1982 - accuracy: 0.9102\n",
            "Epoch 299/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.1963 - accuracy: 0.9132\n",
            "Epoch 300/300\n",
            "62/62 [==============================] - 0s 7ms/step - loss: 0.1994 - accuracy: 0.9051\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(total_words, 64, input_length=max_sequence_len-1))\n",
        "model.add(Bidirectional(LSTM(100)))\n",
        "model.add(Dense(total_words, activation='softmax'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = model.fit(input_sequences, one_hot_labels, epochs=300, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXVFpoREhV6Y"
      },
      "source": [
        "### View the Training Graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "aeSNfS7uhch0",
        "outputId": "1d705134-a43c-4277-f3d8-c4f6249b06ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAGwCAYAAABVdURTAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAABLvklEQVR4nO3deXgTdf4H8HeSJumdtqRN7wMo912glsuDSkFEUVRUVlnWxR8CiqKuoBzq7orXIh4srAe6hwLCiheIC1VQoIKU+yoUCj1oerdp0yNt8v390TYQ20KPtJOm79fz5HnoZCb5zJB23vkeMzIhhAARERGRk5BLXQARERGRPTHcEBERkVNhuCEiIiKnwnBDREREToXhhoiIiJwKww0RERE5FYYbIiIiciouUhfQ0SwWCy5fvgwvLy/IZDKpyyEiIqJmEEKgtLQUwcHBkMuv3TbT5cLN5cuXERYWJnUZRERE1AoZGRkIDQ295jpdLtx4eXkBqD043t7eEldDREREzWEwGBAWFmY9j19Llws39V1R3t7eDDdERESdTHOGlHBAMRERETkVhhsiIiJyKgw3RERE5FQYboiIiMipMNwQERGRU2G4ISIiIqfCcENEREROheGGiIiInArDDRERETkVhhsiIiJyKgw3RERE5FQYboiIiMipMNwQERGRXVTVmKUuAQDDDRERtUG12YIKk2Oc0DpaWr4Rq39MxelsQ4e9Z4XJjMrq5h/vqhozjFU1zV4/RV+KgxcLGywvN9UgNbcMF/ONEEI0eP5SgRF3vLcHI/+aiLM5pc1+v/biInUBRO3NbBHIL6uCzttVshqEENAbKhHo7QqZTCZZHb9VUl6Nc7mliInwbVZdNWYL8sqqEKRxa/N7CyHw2YF0BGlccUsfXZtfr6WqasxIOl+AG7p3g6tS0eHvf7UaswXFFdXQeqpbvO2n+y/hg58u4L0Hh+FIRjH+8dN5rJkRgwEhGus6F/ON+PJIFv4wJgrerspGX6ekvBq/XizEmGgt8kqr8OXhLEwdGoIwP/cm3/vnc3l4csMRKBVyfDlvNAI1jf+OGSqr8dPZPJzVl+Le4WHXfM22MtVYUFxhQoCXK/JKq5CaW4bYKD/I5df/fJ+6bMDus3l4MDYcGrfGj1O9grIqzPjgF1wuqcQb36fgziHBWDV9yHV/j6pqzPhsfzrS8o1wUypw17AQ9An0brAPl4srIJMB+WVV+MvW01C7yPHCbf0w+18HYayqwYL4aJjMFui8XDEtJtS6rRACBy8VIS3PiMziCvxz30UoFXJsfWLMNf8GHsssxtIvT+BoZgkAIKG/Dr0DvZFZWI60AiNOZJWg2lwbaoaG++ClO/rjcnEFonVeOJ9bhqc3HUVpZW2IenvnOayeMeyax6G9yURjEcyJGQwGaDQalJSUwNvb+/obkEM4lF6EXWdyMXtcd3g18ce5KUu/PIF//3IJ7z4wFFMGBzd43mwRqDZboJDLoFTIUWO24H+ncrDlcBb8vdR44ba+MFbVQK1UWP/gHUgrxKaDGbhUWI7oAE+8fOcAKOQyWCwC5/PKEKn1gFJR2zC6/0IBXvnuDI5mFOP/buyOxZP6tv2AoPYP8avbz2BImA8eGRMFjZsSQggUGE3ILq6Et5sLzBaBpAsF2He+ANU1Frw2bRDcVArkl1VB46bE5Hf2IL2wHC/c1hezx3W3OSZ5pVU2JyshBB77zyFsP6lH3yBvPJvQC+Oi/fGvpEvo7u+Bm3oH2NT3y4UCBGlcEdHNw2Z5ZlE5KqvN+M8v6fhk30WoXOQ4tPRWeKpb911LCIGk8wUwC4ExPbWQyWQwVtXgk30XsTk5E3mlVQj1dcPQcB/cMTgEcT26IaOwHHM/PYTjWSUYG63Fv/4wssFJyWwR+OboZXx2IB3GqhoEervi/pHh+PlcHgqMJrwydSA07kpUmMzYcToHLnIZeum8EOzjimc2HcXBi0X4/ehIPBwXec19E0Jg1ie/YldKHu4fEYbFt/W1fs6ySyqwL7UA3TxVqDCZYTJbcGs/HdxVLtYa41YkIre0Cn2DvHEx34iKajNu7u2Pj2eNBAAYq2ow+Z2fcbGgHPcND8Xr9wyGEAKf7LuIrKIKBPm4odpswYc/pyG/rArDwn1wubgSekMlNG5KJPTXIb2wHM9N7IPskkr8detpLIiPhhACi744jvozSGyUH8L83FFQVoWRUd1w28BARHTzwPHMEsz65Ffkl1UBAMb18se//jDyuv+vO07l4IUtxzGqRzcsub0ftJ5qXCow4kK+EeOi/bH6x1ScvFyC5yb2wZdHLuPU5RI8GBuOV7adwaUCI7bMHY1nNx/D6WwDRkT6YsXdA1FSUYNnNh2FoaIaA0M1eOWugRAAjqQXo3egJ6atSUJJRTXC/dwxa3Qk/DxUuH1QMBRyGYQQ+PlcPoorqqFSyPHhzxdw8FIRfN2VMFTWwGwReO/Bobh9UO3fmJ2ncvCXracQ4O2KR8ZE4Z3Ecygur4aLQoZLBeXW/ZTJgBt7+SNK64H/ncxBVnFF8z74V/lo5nDklVbhhzO5OK03IKOw4WvU/9835vODGViy5QRMZguUChmEAGosDaOBp9oFVTVma8j5rf7B3jh52QCZDNjx1Dj0DPBq8b5cS0vO3ww35NBqzBbUWARuemMX9IZKxET44s4hwbiQZ8QjY6IQ5ueOymoz1uw6b/0mNCzCBwn9A+HjrkJqbikmvPUTLAII9HbFf/44EofTi9Hd3wO7U/Lw9dHLyCyqQI1FQKmQ4c4hITh12YBTVzUz+7orUVRejTA/N+xceCOe2XQM3xy9bFPnvJt74JEx3fHUxiPYfTYPWk8V7hsehiitBxZ9cRzmuj8UMhmweU4choX7Ym9qAWosFmsoEELgjL4UUVqP67YkJJ0vwKP/OojSuuZmjZsS82/uie9OZONQenGT293U2x9ZRRU4l1uGKK0H0vKNAACFXIb/PBKLuB7dYKqx4Hcf7seBi4VYdns/TBsWivTCcpzWG/Cnzcesr6V2keO+4WH49y+XIJMBb9wzGPfEhMJiEXhl22l8uCcN3q4u2LHwRui8XZF8qRArtp3BwUtFDepa+7thmDggqMH//foD6Xj/5wswmwUSBgSistqMw+nFOJ9Xhucm9sGoHlq88OVxHK7b58GhGjwUF4lP9qXhRFbjXQVvTR+MFdvOILe0yrps9tgomC3AyChfDAz1wRfJmfg8OaPRk0S9G7r7YVwvf6zbc9F64gYAb1cXGCqvdAO4qxQY31eHHv4ecFcpoFLI0c1TjV0pecgoKkf/YG98vPeidf1eOk9smjMKSecL8OzmK9+G62k91bh/RBhu6u2PqhoLZny4v9H6XrqjP87oS3Ex34ikCwUAaj9/Wx8fi4yicvzfv5Ob3DcAUCpkNiexII0ryiprUFpVA7kMkMlkMFsEbh8UhJ2nc1BZbWnwGpHd3KE3VKKy2oJQXzdkFVdACGDJ5L54838pCPFxQ98gb8hlMggAGYXlOHXZgL5BXjiVbbC+fzcPFd55YCjmfXYIxeXV0HmrkWOosu5TY2exqz/fAKBSyKFUyGC8qguth78HCowmFJdXN3kcnrilJ2aP645F/z2OrcezbZ5zUyrw1fzR2HY8G6t2noPWU4VwP3eczzOipKLp19R6qnDv8DCk5Rmx/aS+0XVclXIIAZjMFkweGISk8wUoMJoQpHHF726IwLbj2bAI4HS2ASoXOUw1V46/h0qB4ZF+cFXK0T9Yg5U7zkImAz774w1QK+X44lAmxvTUIqF/IPamFuDhdfthEcCt/XRYcfdA5JVWYd2eNLgo5Aj1dUOYnzsGh2oQ7ueOzKIKLNhwGEczS9DD3wPn84wwWwQeGROFRZP6YP5nh/D9yRxMHRKMVfcPbfIYtAbDzTUw3DgGi0WgqsYCN9WVk3iuoRL/PZSFHEMlJvTTYc3u8zicXoxRPbrhf6dyGryGm1KB2WOjcCyrBLtS8myeC/FxwxdzR+Hlb041+IPUHD7uStwzLBTfHLts/SMKAI+O6473f7oAF7kM9w4Phb+XK95JPAeg6T+yADB5UBBkAL49lg0/DxX8PdVIqeuXXjK5Lx4ZE4WlX53Af35JR4CXGo+Pj8aDI8OhaKQpvbjchBvf2IWSimoMC/dBaWUNzuWWWZ+XyYBuHmoYKqsBUduEPCTcB+v2pDX4xiWXASOj/PDLhUJoPVX49vGxWP1jKv79yyXrOr/9w/nYTT1wLLMYe1MLGtTWJ9ALhopqXC6ptC6L7xuACf0CseSrEzDVWCCXAe4qF8hkQKivO05nG3BvTCjeuNf2W+VL35y0Oek3RqWQw2S2QO0ih1wmQ8VVYxH8PFRYPKkPhob74EKeERt/zUDimVzr8z0DPHFzb3988HNak6+vcVNi9tgoDAjRYOfpHHz+ayaGhPvg1GUDyq4axxDq6wZ/L7W16d7HXYk5N/bA5wczcCHP2OTrX21GbDh2nMpBbmkVtJ4q5JeZANSegJUKOdxUCuSVViGz6Ergqj/J+7grrSfofkHeNuEcqP1MDAzR4FhmCQaHamCorEFavhGje3aDr7sKQgCDQjUY3VOLx9cfhodagX88NBzbjmUjx1CJnadzcLGutcFVKbcGmbuHheBv9w7GlsNZeH7LcYyL9sfIKD/sPpuHPan51t+H0T27Ye3vYvD4+sMNflevZXyfAGQWVVh/V67mIpehZ4AnzuhL4aV2QUykL3al5CGim7tNy8gdg4NRWlmNH+ved2SUH56Z0BvzPjuEvLqAWx/k/DxU+PSPsfhvcibS8o1IPJMLuQwI8HKF3lAJpUKGwaE+MJrMGBrugwdGhGNgqAaV1WZMeOsnpBeW29Q3c1Qkfr1YiGOZJZjYPxD3jwxDrqEKt/bTwddDBQBIzS3FD2dycSHPiDHRWoyM8oNSLoePu9IablyVCqTmlmH9gXQ8GBuOHv6eAGrH30x6+ydcLCiHTAbMvakHhkf4YWSUHzyuai2c/9khfHus4d/BMD83FJdXo7SyBvfEhOKNewY1u9u8xmyBi0KOXEMlCstN1q61E1kluP3dPRga7oPP/y/O2oJtDww318BwIy1TjQXbjmfjncRzyCyuwPybe2LW6EhsOZyFv3x7GiZzw29/9WaNjsS249nwdVfBQ+2C5KtaAFyVcjx+SzTKTTX48vBlZBVXWL9By2TAvJt64r0fUwEA0QGeyC+rQrCPGx67qQeGhfvC202JFL0Ba3dfgK+7En+a2AdaTzUKymqbevek5uOrI1daa+4fEYZXpw0CAPz521P4aE+a9bVX3jcEWcXlePeHVJy8bMAdg4Px1vQhKK2sxu3v7rGenOpPzEDDb5kAMDjMB7PHRmFMTy183FXW5fXv11vnha/mj4aLXIaP9qTh77vOI657N7x4R38EalxhsQhYhIBL3R+X9386j1e2nUGAlxpLbu+HLw5lYmL/QNw5JAR3/X0vzuhL4aFSWL/Z3tpPhx11obJ++aBQDf772CjoSyqRsOonlJvMGNNTi2idp00QcVMqMOfGHnjvx3M2gerWfjr8ZeoABHipYRG1XVczPtwPracKC2/tjR2n9LhYUI7pI8LwxvcpMFsElkzuiyCNG/adz0c3TzV66Txx8rIBa3adBwDc3Nsfr00bBLlchn8nXcIXhzPh6qLA+w8PR5T2SpdYuakGt7+7BxfyjPBSu+Cr+aMR7ueOP/zzIFL0BtzQvRt2nsqB0WTGDd39cN/wMEwaEGQTwM0WAYVchp/P5eGJ9YcRpfXA/SPDcdfQECgVchQaTdiVkosRkbVdNEIIJF8qwi8XCpBVXIGqmtrBt9kllegTWNtkv+HXDMRG+eGz2TfgjN6A+9YmwWgyQ6mQ4Q+jo/BMQm/rCaLabMHWY9nYeTrH5mT18awR+OJQFnrrPDGulz+mrt4LALgnJhRqFwWGR/piWLiv9f8MqG092PXszQ26zCwWAVldy0y9oxnFmLZmH+QyGbbMG4XPf82AWQgsu70/VC61tQkhbLbRl1TiUoERrkoFBoZoIJfLsPNUDv74r4MAasPWvJt72nTD+Lor0TfIG3vO5aOy2ow5N/VAaWUN7v77XlwsKIerUo73HxqOvan5GN9XhyFhPvj+pB5Dw30Q6uuOC3llCPF1w6P/Ssbus3mQyYAfnr4Jkd3c8d0JPc5kG/B/N/aAh9oFxzNLMOc/yRgS7oNXpg7E3vP56BfkjcirPjMLNx7BF4ezANQGgbfvH4ph4b5ozKnLBnx+MAP9g70xsK6Vw13lghqzBRcLytHD36NdxtudyCrB3/6XgvtHhiOhf2Cj6xQaTVj61Qn876QeFgHc1Msfv1wosP6uDwjxxuY5o+w29uxoRjEGhWrsvr8MN9fAcCOdjb+m47XtKSg0mppcZ2i4D4I0rvjuhB49/D0xNMwHm5Iz0TPAE9sXjLWeqC0Wga3Hs7H+QO3AvDfvHYzRPbUAapu27/r7XuSXmaBUyPBkfC/MvakHPjuQDk+1C+4YHNziX7pfLxbi3rVJ1p83z4nD8Eg/ALV/1FNzy+DjroLWU2V9bSEEMgorEObnZl1WVlWDw+lFKDSaENejG/6ddAnv/pBqfd1X7hoIU40Zf/vfWWuXk7erC3Y/ezN8PVRILyjH+JW7UG0W+OcfRuLGXv7WbX97cvmt+rEpvQK9GgxcvVRgxO3v7kFpZQ1clXIsmtgHM0dFYvfZPHTzUGNAiDcyCivg76W2nuy3n8jGfw9l4aU7+iPYxw15pVU4eLEQXq5KxET4wk2lwEd70vD2zrPQeqkxaUAgFt7a26Y1ylRjQcyfd1j39bfi++rw4czhje7L5uRMWITAvTFhzRosCtR+S35r5zn8LjYCcT26NXjeWFWDimpzqwb2tlZuaSV83FTWkHA4vQg/nsm97sDbD3++gL9sPY2Ibu748embbI7BkYxieKgUiNbZjnk4m1OK5/57DEcyivG3ewfj7mGhv33ZJh3PLIFMBpuByi1ltghMfudnZJdUYsvcUehe1wJxPZcKjHjzf2dx19DgZg0+T75UiPvf/wWTBwZds2vker8zpZXVWP71SYTUfRGqH+vUWZVUVEMIAR93FUrKq3E8qwR5ZZW4qVeAtSXJkTHcXAPDjTROZJXgztV7YbYI6LzVeOiGCARp3LByx1lkFVdAqZDhuYl98MiYKMhkMuSV1ja1u8hlOJJRjMhuHi365UvNLcP3J/W4Y3CwXWZmWCwCo179AXpDJSK7uePHZ26y27eSSwVGpOaWIdzP3Xoy0pdU4sOfL2BTciZKKqrx/kMxmNA/EG98fwarfzyPMT21+M8fY+3y/vVOZJVgT2o+7h4WggCvjptZtmDDYXx15DK8XF0w58YeyCyqwPoD6XCRy/D9U+OsTfDU0KH0Iui8XRHi0/zZa0IIFJdXS3Yyq6yuHRjd1KwteykuN8FD7WLXbhGSVkvO3507hlKnUG224E+bj8FsEZg0IBDvPjDU2gIzLSYUZVU1kAE2fcT+Xle+OQ9tohn4WnoGeKJnQM82115PLpfhvhFheCfxHH53Q4Rdm1sjunk0mFEUqHHFktv7oaSiGpuSM3E8qwS39tNh2/HawYf3Dm/+N+7mGhCiadO38tZ6cUp/DI/0Q0I/HQK8XSGEwM29/aFxUzLYXEdTXSTXIpPJJP2W7qpUdMjU+6u7cqnrYbihdiGEgKGui+PZTcdwKtsAH3clXr5zgDXY1GvtFOCO9sQtPRHfNwADOzAADArVYFNyJo5mliAlpxRp+UaoXOS4pU/A9TfuJHw9VHjohgjrzzKZDBOaGDtARNQcneOsQg7v4MVCfHssG5lF5cgorEBmUTmMJrN1BpFSIcNb9w2xaZHpbFwUcgwK9enQ96x/v2OZxdZWm3HR2hZf64eIqCthuKE22/hrOp7fcsJ6LZerCVE7lfid+4fiZidqbegofYK8oFTIUFxejU/rpmdP+s31YIiIyBbDDbVatdmC1747gw/rpkEn9NdhTLQ/wnzdEOrrjmAfVxgqauCuVrT74EFnpXZRoG+QN45llqDAaIKfhwq39u/4WxUQEXUmDDfUKvqSSsz/7JD1arPzb+6Jpyf0ajDQtrNPnXQE9RdfA4DlU/oxKBIRXQfnyFGLXcgrw+R3fsbBS0XwUrtg7e9i8ExCb4e6IaQzqb+WTUJ/He5o5N5YRERki1+rqcXe2nkOBUYT+gR6Ye3vYmyu6En2d2s/HbY/ORY9/T0ZIImImoHhhloko7Ac2+ru1bTyviEMNh1AJpNZ79tCRETXx24papGP9qTBbBEYG61Fv2CecImIyPEw3FCzWSwCXxzKBFB7d2wiIiJHxHBDzXaxwAhDZQ3ULnLc0L3hTQeJiIgcAcMNNVv9dOT+wd68GR0RETksnqGo2Y5mFgNAh9+CgIiIqCUYbqjZjte13AwK7fg7RxMRETUXww01S43ZghOXGW6IiMjxMdxQs5zLLUNltQWeahd013pKXQ4REVGTGG6oWY5mFAMABoR4Qy7nVXKJiMhxMdxQo/JKq5BeUG79ee/5AgDAiEg/qUoiIiJqFt5+gRr44lAmXthyAgDw3YKxCPdzx77UfADAmJ5aKUsjIiK6LrbckI3vT+qx8POjqKg2o6LajE/2XcRpvQEFRhPcVQoMDfeVukQiIqJrYssN2fj66GUAwOBQDY5mlmBzcia8XGs/Jjd07waVC/MwERE5Np6pyMpiEdbupxcm90MPfw+UVdVg7e7zANglRUREnQPDDVmdyjagqLwanmoXDA33wf+N6wEAqDYLyGTATb39Ja6QiIjo+tgtRVY/n6tttbmhux+UCjnuHR6KXoFeSM0tQ5DGFd39eX0bIiJyfAw3ZLUnNQ/Ale4nmUyGIWE+GBLmI2FVRERELcNuKQIAmGosOHixCAAwJppja4iIqPNiuCEAwOlsA6pqLPBxV6IHu5+IiKgTY7ghAMDh9NpWm6FhPpDJeHsFIiLqvBhuCABwpO7eUUPCeJE+IiLq3BhuCABwuC7cDA33kbQOIiKitmK4IRSUVeFS3U0yB3NmFBERdXIMN2Ttkurh7wGNm1LaYoiIiNpI8nCzevVqREZGwtXVFbGxsThw4MA111+1ahV69+4NNzc3hIWF4amnnkJlZWUHVeuc1h/IAADERHC8DRERdX6ShpuNGzdi4cKFWL58OQ4dOoTBgwcjISEBubm5ja7/2WefYdGiRVi+fDlOnz6Njz76CBs3bsTzzz/fwZU7jx/O5GDn6Ry4yGWYPba71OUQERG1maThZuXKlZg9ezZmzZqFfv36Ye3atXB3d8e6desaXX/fvn0YPXo0HnzwQURGRmLChAl44IEHrtnaU1VVBYPBYPOgWqYaC1765hQA4JExUYjWeUlcERERUdtJFm5MJhOSk5MRHx9/pRi5HPHx8UhKSmp0m1GjRiE5OdkaZi5cuIBt27bhtttua/J9VqxYAY1GY32EhYXZd0c6sc8PZuBSQTm0nmo8Pj5a6nKIiIjsQrJ7S+Xn58NsNkOn09ks1+l0OHPmTKPbPPjgg8jPz8eYMWMghEBNTQ3mzJlzzW6pxYsXY+HChdafDQYDAw6Aymoz3v3hHADg8Vt6wlPN24wREZFzkHxAcUvs2rULr7zyCv7+97/j0KFD+OKLL7B161b8+c9/bnIbtVoNb29vmwcBm5IzkWOoQoiPG+4fybBHRETOQ7Kv61qtFgqFAjk5OTbLc3JyEBgY2Og2S5cuxUMPPYQ//vGPAICBAwfCaDTi0UcfxQsvvAC5vFNlNUntPFV73B+Oi4DaRSFxNURERPYjWRpQqVSIiYlBYmKidZnFYkFiYiLi4uIa3aa8vLxBgFEoak/MQoj2K9bJVFabsT+tAABwU+8AiashIiKyL0kHWixcuBAzZ87E8OHDMXLkSKxatQpGoxGzZs0CADz88MMICQnBihUrAABTpkzBypUrMXToUMTGxiI1NRVLly7FlClTrCGHru9AWiEqqy0I9HZFLx3vAE5ERM5F0nAzffp05OXlYdmyZdDr9RgyZAi2b99uHWScnp5u01KzZMkSyGQyLFmyBFlZWfD398eUKVPw17/+Vapd6JR+OpsHABjXS8s7gBMRkdORiS7Wn2MwGKDRaFBSUtJlBxffunI3zuWWYfWDwzB5UJDU5RAREV1XS87fHIHbxeQYKnEutwwyGTC6ZzepyyEiIrI7hpsu5pcLtQOJ+wd7w8ddJXE1RERE9sdw08XUh5sbothqQ0REzonhpotJOl8bbuJ6MNwQEZFzYrjpQrJLKnCxoBxyGTAiyk/qcoiIiNoFw00XUt8lNSBEA29XpcTVEBERtQ+Gmy7kaEYJAGB4BFttiIjIeTHcdCFnc0oBAH2CvCSuhIiIqP0w3HQh9eGmt47hhoiInBfDTRdRUFaF/DITACCa95MiIiInxnDTRZzNKQMAhPm5wV0l6S3FiIiI2hXDTRdxLpddUkRE1DUw3HQRKfracBPNcENERE6O4aaLOFfXLcWWGyIicnYMN12AEAIpdTOlejHcEBGRk2O46QKyiitQUlENF7kM3f09pC6HiIioXTHcdAHHMmuvTNwnyAuuSoXE1RAREbUvhpsuoD7cDAzxkbYQIiKiDsBw0wUcyywGAAwO1UhbCBERUQdguHFyFovA8bqWm0GhPtIWQ0RE1AEYbpxcWoERpVU1ULvIedsFIiLqEhhunFx9q03/YG8oFfzvJiIi58eznZM7kcUuKSIi6loYbpzcmbrbLvQN4sX7iIioa2C4cXJn9AYAQO9Ab4krISIi6hgMN04sr7QK+WUmyGRALw4mJiKiLoLhxonV3wk8ws8d7ioXiashIiLqGAw3Tqy+S6oPu6SIiKgLYbhxYvWDiXsHcjAxERF1HQw3TiyFM6WIiKgLYrhxUjVmC87m1LfcsFuKiIi6DoYbJ3UutwxVNRZ4ql0Q7ucudTlEREQdhuHGSR3NKAYADAzRQCGXSVsMERFRB2K4cVJHM4sBAIPDfCStg4iIqKMx3DipIxm195QawnBDRERdDMONEyo31VgHEzPcEBFRV8Nw44ROXjbAbBHQeasRqHGVuhwiIqIOxXDjhOoHEw8O9ZG0DiIiIikw3DihtHwjAF6ZmIiIuiaGGyeUY6gCAHZJERFRl8Rw44RyDJUAAJ0Xww0REXU9DDdOyBpuvBluiIio62G4cTI1Zgvyy2q7pXQatcTVEBERdTyGGyeTX2aCRQAKuQzdPBhuiIio62G4cTL1XVIBXmreU4qIiLokhhsnYw03HG9DRERdFMONk7kyU4pdUkRE1DUx3DgZXuOGiIi6OoYbJ8Np4ERE1NUx3DgZ/VUDiomIiLoihhsnk8tuKSIi6uIYbpxMTim7pYiIqGtjuHEixeUmFJdXA+B9pYiIqOtiuHEi353QAwD6BHpB466UuBoiIiJpMNw4kS8PZwEApg4NkbgSIiIi6TDcOInLxRXYn1YIAJgyOFjiaoiIiKTDcOMkth7LBgCMjPJDiI+bxNUQERFJh+HGSRzJLAYAjO8TIG0hREREEmO4cRLnckoBAL0CvSSuhIiISFoMN06g2mxBWr4RABAd4ClxNURERNJiuHEClwqMqDYLeKgUHG9DRERdHsONEzibUwYA6BngCZlMJnE1RERE0mK4cQLn6sJNtI7jbYiIiBhunMDZ3NrBxBxvQ0RExHDjFKwzpdhyQ0REJH24Wb16NSIjI+Hq6orY2FgcOHDgmusXFxdj3rx5CAoKglqtRq9evbBt27YOqtbxXD1TqidbboiIiOAi5Ztv3LgRCxcuxNq1axEbG4tVq1YhISEBKSkpCAhoeDE6k8mEW2+9FQEBAdi8eTNCQkJw6dIl+Pj4dHzxDiKzqALVZgG1i5wzpYiIiCBxuFm5ciVmz56NWbNmAQDWrl2LrVu3Yt26dVi0aFGD9detW4fCwkLs27cPSmXtXa8jIyM7smSHc7GgttUmspsH5HLOlCIiIpKsW8pkMiE5ORnx8fFXipHLER8fj6SkpEa3+frrrxEXF4d58+ZBp9NhwIABeOWVV2A2m5t8n6qqKhgMBpuHM7lY1yUVqXWXuBIiIiLHIFm4yc/Ph9lshk6ns1mu0+mg1+sb3ebChQvYvHkzzGYztm3bhqVLl+Jvf/sb/vKXvzT5PitWrIBGo7E+wsLC7LofUrOGm24eEldCRETkGCQfUNwSFosFAQEBeP/99xETE4Pp06fjhRdewNq1a5vcZvHixSgpKbE+MjIyOrDi9nexoBwAEKlluCEiIgIkHHOj1WqhUCiQk5NjszwnJweBgYGNbhMUFASlUgmFQmFd1rdvX+j1ephMJqhUqgbbqNVqqNVq+xbvQK4ec0NEREQSttyoVCrExMQgMTHRusxisSAxMRFxcXGNbjN69GikpqbCYrFYl509exZBQUGNBhtnV222ILOoAgDH3BAREdWTtFtq4cKF+OCDD/DPf/4Tp0+fxmOPPQaj0WidPfXwww9j8eLF1vUfe+wxFBYWYsGCBTh79iy2bt2KV155BfPmzZNqFySVWVQBs0XAVSmHzstV6nKIiIgcgqRTwadPn468vDwsW7YMer0eQ4YMwfbt262DjNPT0yGXX8lfYWFh+P777/HUU09h0KBBCAkJwYIFC/Dcc89JtQuSunowMaeBExER1ZIJIYTURXQkg8EAjUaDkpISeHt7S11Om3y8Nw0vfXMKCf11+MdDw6Uuh4iIqN205PzdqWZLka0r17jhYGIiIqJ6DDedmHUaOGdKERERWTHcdGKcBk5ERNQQw00ndfU08Ch2SxEREVkx3HRSV08DD/By3osUEhERtRTDTSfFaeBERESNY7jppOrH20R045WJiYiIrsZw00lxGjgREVHjGG46qbS6aeBRnClFRERkg+Gmk7pk7ZZiuCEiIroaw00nxGngRERETWO46YQ4DZyIiKhprQo3P/74o73roBbgNHAiIqKmtSrcTJw4ET169MBf/vIXZGRk2Lsmuo60fN52gYiIqCmtCjdZWVmYP38+Nm/ejO7duyMhIQGff/45TCaTveujRlgHE2t5jRsiIqLfalW40Wq1eOqpp3DkyBHs378fvXr1wty5cxEcHIwnnngCR48etXeddBVOAyciImpamwcUDxs2DIsXL8b8+fNRVlaGdevWISYmBmPHjsXJkyftUSP9Bi/gR0RE1LRWh5vq6mps3rwZt912GyIiIvD999/jvffeQ05ODlJTUxEREYF7773XnrUSAFONBZlFtS03HHNDRETUkEtrNnr88cexfv16CCHw0EMP4fXXX8eAAQOsz3t4eODNN99EcHCw3QqlWplF5bAIwFUph86b08CJiIh+q1Xh5tSpU3j33Xdx9913Q61u/ASr1Wo5ZbwdXCq40mojk3EaOBER0W+1KtwkJiZe/4VdXHDjjTe25uXpGjgNnIiI6NpaNeZmxYoVWLduXYPl69atw2uvvdbmoqhp6YW1LTecBk5ERNS4VoWbf/zjH+jTp0+D5f3798fatWvbXBQ1Lbuk9p5SIT5uEldCRETkmFoVbvR6PYKCghos9/f3R3Z2dpuLoqbpDVUAAJ23q8SVEBEROaZWhZuwsDDs3bu3wfK9e/dyhlQ7yympBMBwQ0RE1JRWDSiePXs2nnzySVRXV+OWW24BUDvI+E9/+hOefvppuxZIV5gtAnlltS03gQw3REREjWpVuHn22WdRUFCAuXPnWu8n5erqiueeew6LFy+2a4F0RUFZFcwWAbkM0HqqpC6HiIjIIbUq3MhkMrz22mtYunQpTp8+DTc3N0RHRzd5zRuyD72htktK66mGi6LNd84gIiJySq0KN/U8PT0xYsQIe9VC15FTN5g4UMMuKSIioqa0OtwcPHgQn3/+OdLT061dU/W++OKLNhdGDdW33HAwMRERUdNa1bexYcMGjBo1CqdPn8aWLVtQXV2NkydP4ocffoBGo7F3jVTnykwpdv8RERE1pVXh5pVXXsFbb72Fb775BiqVCm+//TbOnDmD++67D+Hh4faukerk1LXccKYUERFR01oVbs6fP4/JkycDAFQqFYxGI2QyGZ566im8//77di2QrmC3FBER0fW1Ktz4+vqitLQUABASEoITJ04AAIqLi1FeXm6/6shGDsMNERHRdbVqQPG4ceOwY8cODBw4EPfeey8WLFiAH374ATt27MD48ePtXSPV4WwpIiKi62tVuHnvvfdQWVnbivDCCy9AqVRi3759mDZtGpYsWWLXAqlWZbUZJRXVANhyQ0REdC0tDjc1NTX49ttvkZCQAACQy+VYtGiR3QsjW1nFtXcDd1cp4O3apssTERERObUWj7lxcXHBnDlzrC031DEyCmvHMoX7uUMmk0lcDRERkeNq1YDikSNH4siRI3Yuha6lPtyE+rpLXAkREZFja1X/xty5c7Fw4UJkZGQgJiYGHh4eNs8PGjTILsXRFRlFtd1S4X4MN0RERNfSqnBz//33AwCeeOIJ6zKZTAYhBGQyGcxms32qI6v0gtqWmzA/N4krISIicmytCjdpaWn2roOuI6OoLtywW4qIiOiaWhVuIiIi7F0HXYd1QHE3hhsiIqJraVW4+de//nXN5x9++OFWFUONKymvhqGyBgAQ6stuKSIiomtpVbhZsGCBzc/V1dUoLy+HSqWCu7s7w42d1XdJaT1VcFfxGjdERETX0qqp4EVFRTaPsrIypKSkYMyYMVi/fr29a+zy6rukwjhTioiI6LpaFW4aEx0djVdffbVBqw61XXohBxMTERE1l93CDVB79eLLly/b8yUJQGbdNW44DZyIiOj6WjWA4+uvv7b5WQiB7OxsvPfeexg9erRdCqMrcktrb3URqGG4ISIiup5WhZupU6fa/CyTyeDv749bbrkFf/vb3+xRF10lr7QKAODvqZa4EiIiIsfXqnBjsVjsXQddQ15ZXbjxYrghIiK6HruOuSH7E0Kw5YaIiKgFWhVupk2bhtdee63B8tdffx333ntvm4uiK8qqalBZXdtSpvVSSVwNERGR42tVuPnpp59w2223NVg+adIk/PTTT20uiq6ob7XxVLvwAn5ERETN0KpwU1ZWBpWqYSuCUqmEwWBoc1F0hbVLiuNtiIiImqVV4WbgwIHYuHFjg+UbNmxAv3792lwUXWEdTMzxNkRERM3Sqn6OpUuX4u6778b58+dxyy23AAASExOxfv16bNq0ya4FdnVsuSEiImqZVoWbKVOm4Msvv8Qrr7yCzZs3w83NDYMGDcLOnTtx44032rvGLo3hhoiIqGVaPUJ18uTJmDx5sj1roUYw3BAREbVMq8bc/Prrr9i/f3+D5fv378fBgwfbXBRdwTE3RERELdOqcDNv3jxkZGQ0WJ6VlYV58+a1uSi6gi03RERELdOqcHPq1CkMGzaswfKhQ4fi1KlTbS6KrmC4ISIiaplWhRu1Wo2cnJwGy7Ozs+HiwgvN2YvZIlBgNAFguCEiImquVoWbCRMmYPHixSgpKbEuKy4uxvPPP49bb73VbsV1dUXlJpgtAjIZ4OfBWy8QERE1R6uaWd58802MGzcOERERGDp0KADgyJEj0Ol0+Pe//23XAruy+i4pP3cVlAre45SIiKg5WnXGDAkJwbFjx/D666+jX79+iImJwdtvv43jx48jLCysxa+3evVqREZGwtXVFbGxsThw4ECzttuwYQNkMhmmTp3a4vfsDDjehoiIqOVaPUDGw8MDY8aMQXh4OEym2nEh3333HQDgjjvuaPbrbNy4EQsXLsTatWsRGxuLVatWISEhASkpKQgICGhyu4sXL+KZZ57B2LFjW7sLDo/hhoiIqOVaFW4uXLiAu+66C8ePH4dMJoMQAjKZzPq82Wxu9mutXLkSs2fPxqxZswAAa9euxdatW7Fu3TosWrSo0W3MZjNmzJiBl156CT///DOKi4tbsxsOj9e4ISIiarlWdUstWLAAUVFRyM3Nhbu7O06cOIHdu3dj+PDh2LVrV7Nfx2QyITk5GfHx8VcKkssRHx+PpKSkJrd7+eWXERAQgEceeeS671FVVQWDwWDz6CzYckNERNRyrQo3SUlJePnll6HVaiGXy6FQKDBmzBisWLECTzzxRLNfJz8/H2azGTqdzma5TqeDXq9vdJs9e/bgo48+wgcffNCs91ixYgU0Go310ZoxQVJhuCEiImq5VoUbs9kMLy8vAIBWq8Xly5cBABEREUhJSbFfdb9RWlqKhx56CB988AG0Wm2ztqmfsl7/aOzKyo6K4YaIiKjlWjXmZsCAATh69CiioqIQGxuL119/HSqVCu+//z66d+/e7NfRarVQKBQNLgiYk5ODwMDABuufP38eFy9exJQpU6zLLBZL7Y64uCAlJQU9evSw2UatVkOt7pzhgGNuiIiIWq5VLTdLliyxhoqXX34ZaWlpGDt2LLZt24Z33nmn2a+jUqkQExODxMRE6zKLxYLExETExcU1WL9Pnz44fvw4jhw5Yn3ccccduPnmm3HkyJFO1eXUHGy5ISIiarlWtdwkJCRY/92zZ0+cOXMGhYWF8PX1tZk11RwLFy7EzJkzMXz4cIwcORKrVq2C0Wi0zp56+OGHERISghUrVsDV1RUDBgyw2d7HxwcAGizv7KpqzCipqAbAcENERNQSdrsRlJ+fX6u2mz59OvLy8rBs2TLo9XoMGTIE27dvtw4yTk9Ph1ze9a7Om19We+0gpUIGjZtS4mqIiIg6D5kQQkhdREcyGAzQaDQoKSmBt7e31OU06UhGMaau3otgjSv2LR4vdTlERESSasn5u+s1iXQSHG9DRETUOgw3DorhhoiIqHUYbhwUww0REVHrMNw4qPy6a9xoeY0bIiKiFmG4cVAMN0RERK3DcOOgCoy1U8H9PFQSV0JERNS5MNw4qMK6cNON4YaIiKhFGG4cVH248fNkuCEiImoJhhsHZLYIFJWzW4qIiKg1GG4cUHG5CfXXjfZ1Z7ghIiJqCYYbB1TfJaVxU0Kp4H8RERFRS/DM6YAKOJiYiIio1RhuHFAhp4ETERG1GsONAyqou4BfN86UIiIiajGGGwd05QJ+vDoxERFRSzHcOCBewI+IiKj1GG4cEG+9QERE1HoMNw6osKyu5YZjboiIiFqM4cYBcbYUERFR6zHcOCB2SxEREbUew42DsVx1X6lunC1FRETUYgw3DsZQWQ2zpfbGUr4eSomrISIi6nwYbhxMXmntBfy8XF2gdlFIXA0REVHnw3DjYPSGSgBAsMZN4kqIiIg6J4YbB5NdUhtudBpXiSshIiLqnBhuHIy+LtwEeTPcEBERtQbDjYOpb7kJZMsNERFRqzDcOJicujE3QQw3RERErcJw42DYckNERNQ2DDcORl9SAYDhhoiIqLUYbhxIZbUZReXVAIAgb04FJyIiag2GGwdSP1PKTamAt5uLxNUQERF1Tgw3DkR/1WBimUwmcTVERESdE8ONA9FzMDEREVGbMdw4EOtMKV7Aj4iIqNUYbhwIZ0oRERG1HcONA8kvMwEA/L3UEldCRETUeTHcOJBCY2248fNQSVwJERFR58Vw40CKyhluiIiI2orhxoHUhxtfd4YbIiKi1mK4cRBCCBQZa69O7MuWGyIiolZjuHEQRpMZJrMFAODrrpS4GiIios6L4cZBFNUNJla7yOGmVEhcDRERUefFcOMgrh5MzFsvEBERtR7DjYOonwbuw8HEREREbcJw4yCKy2sHE/t5cLwNERFRWzDcOAi23BAREdkHw42DKK4fc8NwQ0RE1CYMNw6isP4CfrzGDRERUZsw3DgI6wX8eI0bIiKiNmG4cRC8rxQREZF9MNw4CA4oJiIisg+GGwdRxAHFREREdsFw4wCEECgqr79pJsfcEBERtQXDjQMoN5lhqqm/aSZbboiIiNqC4cYB1I+3USnkcFfxpplERERtwXDjALKKKwAAwT6uvGkmERFRGzHcOID0wnIAQJifu8SVEBERdX4MNw4goy7chDPcEBERtRnDjQNgyw0REZH9MNw4gHS23BAREdkNw40DYLcUERGR/TDcSMxYVYP8stqp4OyWIiIiajuGG4llFNW22mjclNC48erEREREbcVwI7H0AnZJERER2RPDjcQ4mJiIiMi+HCLcrF69GpGRkXB1dUVsbCwOHDjQ5LoffPABxo4dC19fX/j6+iI+Pv6a6zu6DE4DJyIisivJw83GjRuxcOFCLF++HIcOHcLgwYORkJCA3NzcRtfftWsXHnjgAfz4449ISkpCWFgYJkyYgKysrA6u3D6yiisBAKG+bhJXQkRE5BwkDzcrV67E7NmzMWvWLPTr1w9r166Fu7s71q1b1+j6n376KebOnYshQ4agT58++PDDD2GxWJCYmNjBldtHjqE23AR6u0pcCRERkXOQNNyYTCYkJycjPj7eukwulyM+Ph5JSUnNeo3y8nJUV1fDz8+v0eerqqpgMBhsHo5EXx9uNAw3RERE9iBpuMnPz4fZbIZOp7NZrtPpoNfrm/Uazz33HIKDg20C0tVWrFgBjUZjfYSFhbW5bnupMVuQX1YFAAjwVktcDRERkXOQvFuqLV599VVs2LABW7Zsgatr4y0fixcvRklJifWRkZHRwVU2La+sCkIALnIZtB4MN0RERPbgIuWba7VaKBQK5OTk2CzPyclBYGDgNbd988038eqrr2Lnzp0YNGhQk+up1Wqo1Y4ZHPQltV1SAV5qyOUyiashIiJyDpK23KhUKsTExNgMBq4fHBwXF9fkdq+//jr+/Oc/Y/v27Rg+fHhHlNoucgz1XVIcb0NERGQvkrbcAMDChQsxc+ZMDB8+HCNHjsSqVatgNBoxa9YsAMDDDz+MkJAQrFixAgDw2muvYdmyZfjss88QGRlpHZvj6ekJT09PyfajNThTioiIyP4kDzfTp09HXl4eli1bBr1ejyFDhmD79u3WQcbp6emQy680MK1ZswYmkwn33HOPzessX74cL774YkeW3mb1M6V0HExMRERkN5KHGwCYP38+5s+f3+hzu3btsvn54sWL7V9QB6lvudFxGjgREZHddOrZUp0du6WIiIjsj+FGQvUDinUMN0RERHbDcCOhnJL6MTcMN0RERPbCcCMRY1UNSqtqAPDWC0RERPbEcCOR+vE2HioFPNUOMa6biIjIKTDcSCS/zAQA8PfiNHAiIiJ7YriRSEHdDTO7eTLcEBER2RPDjUTyjbUtN908VBJXQkRE5FwYbiRS33KjZbcUERGRXTHcSCS/Ptyw5YaIiMiuGG4kUlA3oJhjboiIiOyL4UYiV8INW26IiIjsieFGIvnGum4pttwQERHZFcONRPJL68MNW26IiIjsieFGAqYaCwyVtbde6ObBlhsiIiJ7YriRQGHdNW4Uchk0bkqJqyEiInIuDDcSqJ8G3s1DBblcJnE1REREzoXhRgL5vPUCERFRu2G4kUD9NHAOJiYiIrI/hhsJFHAaOBERUbthuJFAfhlvmklERNReGG4kwDE3RERE7YfhRgKZhRUAgGAfV4krISIicj4MNxI4n1cGAOiu9ZS4EiIiIufDcNPBSsqrUVB3Eb8ofw+JqyEiInI+DDcd7Hx+batNoLcrPNUuEldDRETkfBhuOtiFPCMAoDtbbYiIiNoFw00Hu1A/3obhhoiIqF0w3HQwDiYmIiJqXww3Hay+W6pHAMMNERFRe2C46UBmi8ClgnIAQHctu6WIiIjaA8NNB0ovLIfJbIHaRY4QHzepyyEiInJKDDcd6NClIgBA3yBvyOUyiashIiJyTgw3HejgpUIAwMgoP4krISIicl4MNx3oQFptuBkRyXBDRETUXhhuOkhBWRXO182UGh7hK3E1REREzovhpoMcrBtvEx3gCV8PlcTVEBEROS+Gmw7ya32XFMfbEBERtSuGmw5yPKsEADA0zEfaQoiIiJwcw00HOZdbe9uFPoHeEldCRETk3BhuOkBBWRUKjSYAQI8AXpmYiIioPTHcdICzObWtNmF+bnBXuUhcDRERkXNjuOkAqbmlAIBeAV4SV0JEROT8GG46QH3LTU8d7wRORETU3hhuOsDZHLbcEBERdRSGmw6QWjdTqpeO4YaIiKi9Mdy0s4KyKhRwphQREVGHYbhpZztP5wAAevh7cKYUERFRB2C4aUdCCPxz3yUAwPQRYRJXQ0RE1DUw3LSj5EtFOJVtgNpFjvuGM9wQERF1BIabdvTPpNpWm6lDQuDjzjuBExERdQSGm3aSY6jEd8ezAQAPxUVIXA0REVHXwRGudvb+T+fx8d6L6BngiRqLwPAIXwwI0UhdFhERUZfBcGNHmUXleOP7FFSbBbJLKgEAD4+KlLYoIiKiLobdUnb0TuI5VJsFNG5KAECgtysm9g+UuCoiIqKuhS03dnI+rwybkzMBAB/PGoGSimqE+7lD5cL8SERE1JEYbuwku7gS/l5qDAzRYFi4r9TlEBERdVkMN3YyJlqL3c/eDENFtdSlEBERdWkMN3bkqlTAVamQugwiIqIujQNCiIiIyKkw3BAREZFTYbghIiIip8JwQ0RERE6F4YaIiIicCsMNERERORWHCDerV69GZGQkXF1dERsbiwMHDlxz/U2bNqFPnz5wdXXFwIEDsW3btg6qlIiIiByd5OFm48aNWLhwIZYvX45Dhw5h8ODBSEhIQG5ubqPr79u3Dw888AAeeeQRHD58GFOnTsXUqVNx4sSJDq6ciIiIHJFMCCGkLCA2NhYjRozAe++9BwCwWCwICwvD448/jkWLFjVYf/r06TAajfj222+ty2644QYMGTIEa9euve77GQwGaDQalJSUwNvb2347QkRERO2mJedvSVtuTCYTkpOTER8fb10ml8sRHx+PpKSkRrdJSkqyWR8AEhISmly/qqoKBoPB5kFERETOS9Jwk5+fD7PZDJ1OZ7Ncp9NBr9c3uo1er2/R+itWrIBGo7E+wsLC7FM8EREROSTJx9y0t8WLF6OkpMT6yMjIkLokIiIiakeS3jhTq9VCoVAgJyfHZnlOTg4CAwMb3SYwMLBF66vVaqjVavsUTERERA5P0nCjUqkQExODxMRETJ06FUDtgOLExETMnz+/0W3i4uKQmJiIJ5980rpsx44diIuLa9Z71o+f5tgbIiKizqP+vN2seVBCYhs2bBBqtVp88skn4tSpU+LRRx8VPj4+Qq/XCyGEeOihh8SiRYus6+/du1e4uLiIN998U5w+fVosX75cKJVKcfz48Wa9X0ZGhgDABx988MEHH3x0wkdGRsZ1z/WSttwAtVO78/LysGzZMuj1egwZMgTbt2+3DhpOT0+HXH5laNCoUaPw2WefYcmSJXj++ecRHR2NL7/8EgMGDGjW+wUHByMjIwNeXl6QyWR23ReDwYCwsDBkZGRwmvl18Fi1DI9X8/FYNR+PVcvweDVfexwrIQRKS0sRHBx83XUlv86NM+E1dJqPx6pleLyaj8eq+XisWobHq/mkPlZOP1uKiIiIuhaGGyIiInIqDDd2pFarsXz5ck49bwYeq5bh8Wo+Hqvm47FqGR6v5pP6WHHMDRERETkVttwQERGRU2G4ISIiIqfCcENEREROheGGiIiInArDjZ2sXr0akZGRcHV1RWxsLA4cOCB1SQ7hxRdfhEwms3n06dPH+nxlZSXmzZuHbt26wdPTE9OmTWtwY1Rn9dNPP2HKlCkIDg6GTCbDl19+afO8EALLli1DUFAQ3NzcEB8fj3PnztmsU1hYiBkzZsDb2xs+Pj545JFHUFZW1oF70TGud6x+//vfN/icTZw40WadrnKsVqxYgREjRsDLywsBAQGYOnUqUlJSbNZpzu9deno6Jk+eDHd3dwQEBODZZ59FTU1NR+5Kh2jO8brpppsafL7mzJljs05XOF5r1qzBoEGD4O3tDW9vb8TFxeG7776zPu9InyuGGzvYuHEjFi5ciOXLl+PQoUMYPHgwEhISkJubK3VpDqF///7Izs62Pvbs2WN97qmnnsI333yDTZs2Yffu3bh8+TLuvvtuCavtOEajEYMHD8bq1asbff7111/HO++8g7Vr12L//v3w8PBAQkICKisrrevMmDEDJ0+exI4dO/Dtt9/ip59+wqOPPtpRu9BhrnesAGDixIk2n7P169fbPN9VjtXu3bsxb948/PLLL9ixYweqq6sxYcIEGI1G6zrX+70zm82YPHkyTCYT9u3bh3/+85/45JNPsGzZMil2qV0153gBwOzZs20+X6+//rr1ua5yvEJDQ/Hqq68iOTkZBw8exC233II777wTJ0+eBOBgn6uW3uiSGho5cqSYN2+e9Wez2SyCg4PFihUrJKzKMSxfvlwMHjy40eeKi4uFUqkUmzZtsi47ffq0ACCSkpI6qELHAEBs2bLF+rPFYhGBgYHijTfesC4rLi4WarVarF+/XgghxKlTpwQA8euvv1rX+e6774RMJhNZWVkdVntH++2xEkKImTNnijvvvLPJbbrqsRJCiNzcXAFA7N69WwjRvN+7bdu2Cblcbr2BsRBCrFmzRnh7e4uqqqqO3YEO9tvjJYQQN954o1iwYEGT23Tl4+Xr6ys+/PBDh/tcseWmjUwmE5KTkxEfH29dJpfLER8fj6SkJAkrcxznzp1DcHAwunfvjhkzZiA9PR0AkJycjOrqaptj16dPH4SHh3f5Y5eWlga9Xm9zbDQaDWJjY63HJikpCT4+Phg+fLh1nfj4eMjlcuzfv7/Da5barl27EBAQgN69e+Oxxx5DQUGB9bmufKxKSkoAAH5+fgCa93uXlJSEgQMHWm9gDAAJCQkwGAzWb+nO6rfHq96nn34KrVaLAQMGYPHixSgvL7c+1xWPl9lsxoYNG2A0GhEXF+dwnyvJ7wre2eXn58NsNtv8ZwGATqfDmTNnJKrKccTGxuKTTz5B7969kZ2djZdeegljx47FiRMnoNfroVKp4OPjY7ONTqeDXq+XpmAHUb//jX2u6p/T6/UICAiwed7FxQV+fn5d7vhNnDgRd999N6KionD+/Hk8//zzmDRpEpKSkqBQKLrssbJYLHjyyScxevRoDBgwAACa9Xun1+sb/ezVP+esGjteAPDggw8iIiICwcHBOHbsGJ577jmkpKTgiy++ANC1jtfx48cRFxeHyspKeHp6YsuWLejXrx+OHDniUJ8rhhtqV5MmTbL+e9CgQYiNjUVERAQ+//xzuLm5SVgZOZP777/f+u+BAwdi0KBB6NGjB3bt2oXx48dLWJm05s2bhxMnTtiMc6OmNXW8rh6bNXDgQAQFBWH8+PE4f/48evTo0dFlSqp37944cuQISkpKsHnzZsycORO7d++WuqwG2C3VRlqtFgqFosGI8JycHAQGBkpUlePy8fFBr169kJqaisDAQJhMJhQXF9usw2MH6/5f63MVGBjYYNB6TU0NCgsLu/zx6969O7RaLVJTUwF0zWM1f/58fPvtt/jxxx8RGhpqXd6c37vAwMBGP3v1zzmjpo5XY2JjYwHA5vPVVY6XSqVCz549ERMTgxUrVmDw4MF4++23He5zxXDTRiqVCjExMUhMTLQus1gsSExMRFxcnISVOaaysjKcP38eQUFBiImJgVKptDl2KSkpSE9P7/LHLioqCoGBgTbHxmAwYP/+/dZjExcXh+LiYiQnJ1vX+eGHH2CxWKx/fLuqzMxMFBQUICgoCEDXOlZCCMyfPx9btmzBDz/8gKioKJvnm/N7FxcXh+PHj9sEwh07dsDb2xv9+vXrmB3pINc7Xo05cuQIANh8vrrK8foti8WCqqoqx/tc2XV4che1YcMGoVarxSeffCJOnTolHn30UeHj42MzIryrevrpp8WuXbtEWlqa2Lt3r4iPjxdarVbk5uYKIYSYM2eOCA8PFz/88IM4ePCgiIuLE3FxcRJX3TFKS0vF4cOHxeHDhwUAsXLlSnH48GFx6dIlIYQQr776qvDx8RFfffWVOHbsmLjzzjtFVFSUqKiosL7GxIkTxdChQ8X+/fvFnj17RHR0tHjggQek2qV2c61jVVpaKp555hmRlJQk0tLSxM6dO8WwYcNEdHS0qKystL5GVzlWjz32mNBoNGLXrl0iOzvb+igvL7euc73fu5qaGjFgwAAxYcIEceTIEbF9+3bh7+8vFi9eLMUutavrHa/U1FTx8ssvi4MHD4q0tDTx1Vdfie7du4tx48ZZX6OrHK9FixaJ3bt3i7S0NHHs2DGxaNEiIZPJxP/+9z8hhGN9rhhu7OTdd98V4eHhQqVSiZEjR4pffvlF6pIcwvTp00VQUJBQqVQiJCRETJ8+XaSmplqfr6ioEHPnzhW+vr7C3d1d3HXXXSI7O1vCijvOjz/+KAA0eMycOVMIUTsdfOnSpUKn0wm1Wi3Gjx8vUlJSbF6joKBAPPDAA8LT01N4e3uLWbNmidLSUgn2pn1d61iVl5eLCRMmCH9/f6FUKkVERISYPXt2gy8XXeVYNXacAIiPP/7Yuk5zfu8uXrwoJk2aJNzc3IRWqxVPP/20qK6u7uC9aX/XO17p6eli3Lhxws/PT6jVatGzZ0/x7LPPipKSEpvX6QrH6w9/+IOIiIgQKpVK+Pv7i/Hjx1uDjRCO9bmSCSGEfduCiIiIiKTDMTdERETkVBhuiIiIyKkw3BAREZFTYbghIiIip8JwQ0RERE6F4YaIiIicCsMNERERORWGGyIiInIqDDdE1CXJZDJ8+eWXUpdBRO2A4YaIOtzvf/97yGSyBo+JEydKXRoROQEXqQsgoq5p4sSJ+Pjjj22WqdVqiaohImfClhsikoRarUZgYKDNw9fXF0Btl9GaNWswadIkuLm5oXv37ti8ebPN9sePH8ctt9wCNzc3dOvWDY8++ijKysps1lm3bh369+8PtVqNoKAgzJ8/3+b5/Px83HXXXXB3d0d0dDS+/vpr63NFRUWYMWMG/P394ebmhujo6AZhjIgcE8MNETmkpUuXYtq0aTh69ChmzJiB+++/H6dPnwYAGI1GJCQkwNfXF7/++is2bdqEnTt32oSXNWvWYN68eXj00Udx/PhxfP311+jZs6fNe7z00ku47777cOzYMdx2222YMWMGCgsLre9/6tQpfPfddzh9+jTWrFkDrVbbcQeAiFrP7vcZJyK6jpkzZwqFQiE8PDxsHn/961+FEEIAEHPmzLHZJjY2Vjz22GNCCCHef/994evrK8rKyqzPb926VcjlcqHX64UQQgQHB4sXXnihyRoAiCVLllh/LisrEwDEd999J4QQYsqUKWLWrFn22WEi6lAcc0NEkrj55puxZs0am2V+fn7Wf8fFxdk8FxcXhyNHjgAATp8+jcGDB8PDw8P6/OjRo2GxWJCSkgKZTIbLly9j/Pjx16xh0KBB1n97eHjA29sbubm5AIDHHnsM06ZNw6FDhzBhwgRMnToVo0aNatW+ElHHYrghIkl4eHg06CayFzc3t2atp1QqbX6WyWSwWCwAgEmTJuHSpUvYtm0bduzYgfHjx2PevHl488037V4vEdkXx9wQkUP65ZdfGvzct29fAEDfvn1x9OhRGI1G6/N79+6FXC5H79694eXlhcjISCQmJrapBn9/f8ycORP/+c9/sGrVKrz//vttej0i6hhsuSEiSVRVVUGv19ssc3FxsQ7a3bRpE4YPH44xY8bg008/xYEDB/DRRx8BAGbMmIHly5dj5syZePHFF5GXl4fHH38cDz30EHQ6HQDgxRdfxJw5cxAQEIBJkyahtLQUe/fuxeOPP96s+pYtW4aYmBj0798fVVVV+Pbbb63hiogcG8MNEUli+/btCAoKslnWu3dvnDlzBkDtTKYNGzZg7ty5CAoKwvr169GvXz8AgLu7O77//nssWLAAI0aMgLu7O6ZNm4aVK1daX2vmzJmorKzEW2+9hWeeeQZarRb33HNPs+tTqVRYvHgxLl68CDc3N4wdOxYbNmyww54TUXuTCSGE1EUQEV1NJpNhy5YtmDp1qtSlEFEnxDE3RERE5FQYboiIiMipcMwNETkc9pYTUVuw5YaIiIicCsMNERERORWGGyIiInIqDDdERETkVBhuiIiIyKkw3BAREZFTYbghIiIip8JwQ0RERE7l/wEPVjCk19Se4gAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "  plt.plot(history.history[string])\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(string)\n",
        "  plt.show()\n",
        "\n",
        "plot_graphs(history, 'accuracy')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rAgRpxYhjpB"
      },
      "source": [
        "### Generate new lyrics!\n",
        "\n",
        "It's finally time to generate some new lyrics from the trained model, and see what we get. To do so, we'll provide some \"seed text\", or an input sequence for the model to start with. We'll also decide just how long of an output sequence we want - this could essentially be infinite, as the input plus the previous output will be continuously fed in for a new output word (at least up to our max sequence length)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DC7zfcgviDTp",
        "outputId": "c6028395-06cf-4f1c-dca2-4fd9136e6f76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 640ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 16ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "back in black, I here to sang youd youre late that ship is sure to wait wait wait come stood no smile and look of best father heartaches last deck cassandra make me again again sound sound sound more heartaches walls kisses walls kisses kisses something kisses guy body sleep stood stood as new still alone now over you tired playing break break final aching deck once new new tired heartaches walls final aching must deck must sleep other sleep other deck happy alive cassandra holds holds as saw good as new sleep easy cassandra holds as good as new walking gently chills stood cassandra holds as\n"
          ]
        }
      ],
      "source": [
        "seed_text = \"back in black, I here to sang\"\n",
        "next_words = 100\n",
        "  \n",
        "for _ in range(next_words):\n",
        "\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "\tpredicted = np.argmax(model.predict(token_list), axis=-1)\n",
        "\toutput_word = \"\"\n",
        "\tfor word, index in tokenizer.word_index.items():\n",
        "\t\tif index == predicted:\n",
        "\t\t\toutput_word = word\n",
        "\t\t\tbreak\n",
        "\tseed_text += \" \" + output_word\n",
        "print(seed_text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}